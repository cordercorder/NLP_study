{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 矩阵求导\n",
    "\n",
    "（以下知识点大部分都是从各个地方搬运过来，方便自己理解，~~其它都挺好的，就是写latex公式比较麻烦，为啥其它地方的公式都是图片...~~）\n",
    "\n",
    "### 标量对矩阵的求导\n",
    "\n",
    "这里使用小写字母$x$来表示标量,$\\vec{x}$表示（列）向量，大写字母$\\vec{X}$表示矩阵。\n",
    "\n",
    "首先来琢磨一下定义，标量$f$对矩阵$\\vec{X}$的导数，定义为$\\frac{\\partial f}{\\partial \\vec{X}} = \\left [ \\frac{\\partial f}{\\partial \\vec{X_{ij}}} \\right ]$，即$f$对矩阵$\\vec{X}$逐元素求导排成与矩阵$\\vec{X}$尺寸相同的矩阵。然而，这个定义在计算中并不好用，实用上的原因是对函数较复杂的情形难以逐元素求导；哲理上的原因是逐元素求导破坏了**整体性**，试想，为何要将$f$看做矩阵$\\vec{X}$而不是各元素$\\vec{X_{ij}}$的函数呢？答案是用矩阵运算更整洁。所以在求导时不宜拆开矩阵，而是要找一个从整体出发的算法。\n",
    "\n",
    "为此，我们来回顾，一元微积分中的导数（标量对标量的导数）与微分有联系：$df = {f}'(x)dx$，多元微积分中的梯度（标量对向量的导数）也与微分有联系：$df = \\sum_{i=1}^{n}\\frac{\\partial f}{\\partial x_i} d x_i = (\\frac{\\partial f}{\\partial \\vec{x}} )^T d\\vec{x}$，这里第一个等号是全微分公式，第二个等号表达了梯度与微分的联系：全微分$df$是梯度向量$\\frac{\\partial f}{\\partial \\vec{x}}$$(shape: \\ n \\times 1)$与微分向量$d \\vec{x}$$(shape: \\ n \\times 1)$的内积，受此启发，我们将矩阵导数与微分建立联系：\n",
    "\n",
    "$df = \\sum_{i=1}^{m}\\sum_{j=1}^{n} \\frac{\\partial f}{\\partial \\vec{X_{ij}}} d\\vec{X_{ij}} = tr\\left ( (\\frac{\\partial f}{\\partial \\vec{X}} )^T d\\vec{X} \\right )$，其中$tr$代表迹$(trace)$是方阵对角线元素之和，满足性质：对尺寸相同的矩阵$A$, $B$，$tr(A^T B) = \\sum_{i,j}A_{ij}B_{ij}$，与梯度相似，这里第一个等号是全微分公式，第二个等号表达了矩阵导数与微分的联系：全微分$df$是导数$\\frac{\\partial f}{\\partial \\vec{X}}$$(shape: \\ m \\times n)$与微分矩阵$d \\vec{X}$$(shape: \\ m \\times n)$的内积。\n",
    "\n",
    "然后来建立运算法则。回想遇到较复杂的一元函数如$f = log(2 + sin(x))^{e^{\\sqrt{x}}}$，我们是如何求导的呢？通常不是从定义开始求极限，而是先建立了初等函数求导和四则运算、复合等法则，再来运用这些法则。故而，我们来创立常用的矩阵微分的运算法则：\n",
    "\n",
    "1. 加减法：$d(\\vec{X} \\pm \\vec{Y}) = d(\\vec{X}) \\pm d(\\vec{Y})$；矩阵乘法：$d(\\vec{X} \\vec{Y}) = d(\\vec{X}) \\vec{Y} + \\vec{X}d(\\vec{Y})$；转置：$d({\\vec{X}}^T) = (d(\\vec{X}))^T$；迹：$dtr(\\vec{X}) = tr(d(\\vec{X}))$\n",
    "\n",
    "2. 逆：$d({\\vec{X}}^{-1}) = -{\\vec{X}}^{-1}d(\\vec{X}){\\vec{X}}^{-1}$，证明：$\\vec{X} {\\vec{X}}^{-1} = \\vec{I}$，对两边求微分得：$d(\\vec{X}){\\vec{X}}^{-1} + \\vec{X}d({\\vec{X}}^{-1}) = 0$，故$\\vec{X}d({\\vec{X}}^{-1}) = -d(\\vec{X}){\\vec{X}}^{-1}$，故$d({\\vec{X}}^{-1}) = -{\\vec{X}}^{-1}d(\\vec{X}){\\vec{X}}^{-1}$\n",
    "\n",
    "3. 行列式：$d(|{\\vec{X}}|) = tr({\\vec{X}}^{\\star})$，其中${\\vec{X}}^{\\star}$表示$\\vec{X}$的伴随矩阵，在$\\vec{X}$可逆时又可以写作$d(|{\\vec{X}}|) = |{\\vec{X}}| tr({\\vec{X}}^{-1} d(\\vec{X}))$。此式可用Laplace展开来证明，详见张贤达《矩阵分析与应用》第279页\n",
    "\n",
    "4. 逐元素乘法：$d(\\vec{X} \\odot \\vec{Y}) = d(\\vec{X}) \\odot \\vec{Y} + \\vec{X} \\odot d(\\vec{Y})$，$\\odot$表示尺寸相同的矩阵$\\vec{X}$,$\\vec{Y}$逐元素相乘\n",
    "\n",
    "5. 逐元素函数：$d \\sigma(\\vec{X}) = {\\sigma}'(\\vec{X}) \\odot d(\\vec{X})$，$\\sigma(\\vec{X}) = \\left [ \\sigma(\\vec{X_{ij}}) \\right ]$是逐元素标量函数运算，${\\sigma}'(\\vec{X}) = \\left [ {\\sigma}'(\\vec{X_{ij}}) \\right ]$是逐元素求导数。例如：\n",
    "$\n",
    "\\vec{X} = \\begin{bmatrix}\n",
    "{\\vec{X}}_{11} & {\\vec{X}}_{12}\\\\ \n",
    "{\\vec{X}}_{21} & {\\vec{X}}_{22}\n",
    "\\end{bmatrix}\n",
    "$，$dsin(\\vec{X}) = \\begin{bmatrix}\n",
    "cos({\\vec{X}}_{11})d{\\vec{X}}_{11} & cos({\\vec{X}}_{12})d{\\vec{X}}_{12}\\\\ \n",
    "cos({\\vec{X}}_{21})d{\\vec{X}}_{21} & cos({\\vec{X}}_{22})d{\\vec{X}}_{22}\n",
    "\\end{bmatrix} = cos(\\vec{X}) \\odot d{\\vec{X}}$，我们试图利用矩阵导数与微分的联系$df = tr\\left ( (\\frac{\\partial f}{\\partial \\vec{X}} )^T d\\vec{X} \\right )$，在求出左侧的微分$df$后，该如何写成右侧的形式并得到导数呢？这需要一些迹技巧(trace trick)：\n",
    "\n",
    "\n",
    "1. 标量套上迹：$a = tr(a)$\n",
    "\n",
    "2. 转置：$tr({\\vec{A}}^T) = tr(\\vec{A})$\n",
    "\n",
    "3. 线性：$tr(\\vec{A} \\pm \\vec{B}) = tr(\\vec{A}) \\pm tr(\\vec{B})$\n",
    "\n",
    "4. 矩阵乘法交换：$tr(\\vec{A} \\vec{B}) = tr(\\vec{B} \\vec{A})$，其中$\\vec{A}$与${\\vec{B}}^T$尺寸相同，两侧都等于$\\sum_{i, j} {\\vec{A}}_{ij} {\\vec{B}}_{ji}$\n",
    "\n",
    "5. 矩阵乘法/逐元素乘法交换：$tr({\\vec{A}}^T (\\vec{B} \\odot \\vec{C})) = tr((\\vec{A} \\odot \\vec{B})^T \\vec{C})$，其中$\\vec{A}, \\vec{B}, \\vec{C}$尺寸相同，两侧都等于$\\sum_{ij} {\\vec{A}}_{ij} {\\vec{B}}_{ij} {\\vec{C}}_{ij}$\n",
    "\n",
    "观察一下可以断言，若标量函数$f$是矩阵$\\vec{X}$经加减乘法、逆、行列式、逐元素函数等运算构成，则使用相应的运算法则对$f$求微分，再使用迹技巧给$df$套上迹并将其它项交换至$d\\vec{X}$左侧，对照导数与微分的联系$df = tr\\left ( (\\frac{\\partial f}{\\partial \\vec{X}} )^T d\\vec{X} \\right )$，即能得到导数。\n",
    "\n",
    "特别地，若矩阵退化为向量，对照导数与微分的联系$df = (\\frac{\\partial f}{\\partial \\vec{x}} )^T d\\vec{x}$，即能得到导数\n",
    "\n",
    "在建立法则的最后，来谈一谈复合：假设已求得$\\frac{\\partial f}{\\partial \\vec{Y}}$，而$\\vec{Y}$是$\\vec{X}$的函数，如何求$\\frac{\\partial f}{\\partial \\vec{X}}$呢，在微积分中有标量求导的链式法则$\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial y} \\frac{\\partial y}{\\partial x}$，但这里我们不能随意**沿用标量的链式法则**，因为矩阵对矩阵的导数$\\frac{\\partial \\vec{Y}}{\\partial \\vec{X}}$截至目前仍是未定义的。于是我们继续追本溯源，链式法则是从何而来？源头仍然是微分。我们直接从微分入手建立复合法则：先写出$df = tr\\left ( (\\frac{\\partial f}{\\partial \\vec{Y}} )^T d\\vec{Y} \\right )$，再将$d\\vec{Y}$用$d\\vec{X}$表示出来代入，并使用迹技巧将其他项交换至$d\\vec{X}$左侧，即可得到$\\frac{\\partial f}{\\partial \\vec{X}}$\n",
    "\n",
    "最常见的情形是$\\vec{Y} = \\vec{A} \\vec{X} \\vec{B}$，此时，$d\\vec{Y} = d(\\vec{A}) \\vec{X} \\vec{B} + \\vec{A} d(\\vec{X}) \\vec{B} + \\vec{A} \\vec{X} d(\\vec{B})$，由于$d(\\vec{A}) = 0, d(\\vec{B}) = 0$，故$d\\vec{Y} = \\vec{A} d(\\vec{X}) \\vec{B}$\n",
    "\n",
    "$df = tr\\left ( (\\frac{\\partial f}{\\partial \\vec{Y}} )^T d\\vec{Y} \\right ) = tr\\left ( (\\frac{\\partial f}{\\partial \\vec{Y}} )^T \\vec{A} d(\\vec{X}) \\vec{B} \\right ) = tr\\left ( \\vec{B} (\\frac{\\partial f}{\\partial \\vec{Y}} )^T \\vec{A} d(\\vec{X}) \\right ) = tr\\left ( ({\\vec{A}}^T (\\frac{\\partial f}{\\partial \\vec{Y}} ) {\\vec{B}}^T)^T d(\\vec{X}) \\right )$，可得到$\\frac{\\partial f}{\\partial \\vec{X}} = {\\vec{A}}^T (\\frac{\\partial f}{\\partial \\vec{Y}} ) {\\vec{B}}^T$\n",
    "\n",
    "接下来演示一些算例。特别提醒要依据已经建立的运算法则来计算，不能随意套用微积分中标量导数的结论，比如认为$\\vec{A} \\vec{X}$对$\\vec{X}$的导数为$\\vec{A}$，这是没有根据、意义不明的。\n",
    "\n",
    "**例1**：$f = {\\vec{a}}^T \\vec{X} \\vec{b}$，求$\\frac{\\partial f}{\\partial \\vec{X}}$，其中$\\vec{a}$是$m \\times 1$列向量，$\\vec{X}$是$m \\times n$矩阵，$\\vec{b}$是$n \\times 1$列向量，$f$是标量\n",
    "\n",
    "解：先使用矩阵乘法法则求微分，$df = d({\\vec{a}}^T) \\vec{X} \\vec{b} + {\\vec{a}}^T d(\\vec{X}) \\vec{b} + {\\vec{a}}^T \\vec{X} d(\\vec{b})$，由于$d({\\vec{a}}^T) = 0, d(\\vec{b}) = 0$，故$df = {\\vec{a}}^T d(\\vec{X}) \\vec{b}$，由于$df$是标量,因此$df = tr(df)$，$tr(df) = df = tr\\left ( {\\vec{a}}^T d(\\vec{X}) \\vec{b} \\right ) = tr\\left ( \\vec{b} {\\vec{a}}^T d(\\vec{X}) \\right ) = tr\\left (({\\vec{a}} {\\vec{b}}^T)^T d(\\vec{X}) \\right ) = tr\\left ( (\\frac{\\partial f}{\\partial \\vec{X}} )^T d\\vec{X} \\right )$，故$\\frac{\\partial f}{\\partial \\vec{X}} = {\\vec{a}} {\\vec{b}}^T$。\n",
    "\n",
    "注意：这里不能用$\\frac{\\partial f}{\\partial \\vec{X}} = {\\vec{a}}^T \\frac{\\partial \\vec{X}}{\\partial \\vec{X}} \\vec{b}$，导数与矩阵乘法的交换是不合法则的运算（而微分是合法的）。有些资料在计算矩阵导数时，会略过求微分这一步，这是逻辑上解释不通的。\n",
    "\n",
    "**例2**：$f = {\\vec{a}}^T exp(\\vec{X} \\vec{b})$，求$\\frac{\\partial f}{\\partial \\vec{X}}$，其中$\\vec{a}$是$m \\times 1$列向量，$\\vec{X}$是$m \\times n$矩阵，$\\vec{b}$是$n \\times 1$列向量，$exp$表示逐元素求指数，$f$是标量\n",
    "\n",
    "解：先使用矩阵乘法、逐元素函数法则求微分：$df = d({\\vec{a}}^T) exp(\\vec{X} \\vec{b}) + {\\vec{a}}^T d(exp(\\vec{X} \\vec{b})) = {\\vec{a}}^T d(exp(\\vec{X} \\vec{b})) = {\\vec{a}}^T (exp(\\vec{X} \\vec{b}) \\odot d(\\vec{X} \\vec{b})) = {\\vec{a}}^T (exp(\\vec{X} \\vec{b}) \\odot (d(\\vec{X}) \\vec{b} + \\vec{X} d(\\vec{b}))) = {\\vec{a}}^T (exp(\\vec{X} \\vec{b}) \\odot (d(\\vec{X}) \\vec{b}))$，由于$df$是标量，$tr(df) = df = tr \\left ( {\\vec{a}}^T (exp(\\vec{X} \\vec{b}) \\odot (d(\\vec{X}) \\vec{b})) \\right ) = tr\\left ( ({\\vec{a}}^T \\odot exp(\\vec{X} \\vec{b}))^T (d(\\vec{X}) \\vec{b}) \\right ) = tr\\left ( \\vec{b} ({\\vec{a}}^T \\odot exp(\\vec{X} \\vec{b}))^T d(\\vec{X}) \\right) = tr\\left (  (({\\vec{a}}^T \\odot exp(\\vec{X} \\vec{b})) {\\vec{b}}^T)^T d(\\vec{X}) \\right)$，对照导数与微分的联系$df = tr\\left ( (\\frac{\\partial f}{\\partial \\vec{X}} )^T d\\vec{X} \\right )$，故$\\frac{\\partial f}{\\partial \\vec{X}} = ({\\vec{a}}^T \\odot exp(\\vec{X} \\vec{b})) {\\vec{b}}^T$\n",
    "\n",
    "以后有时间继续加..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, optimizers, datasets\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # or any {'0', '1', '2'}\n",
    "\n",
    "def mnist_dataset():\n",
    "    (x, y), (x_test, y_test) = datasets.mnist.load_data()\n",
    "    #normalize\n",
    "    x = x/255.0\n",
    "    x_test = x_test/255.0\n",
    "    return (x, y), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       "  \n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       "  \n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       "  \n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       "  \n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]]]),\n",
       "  array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)),\n",
       " (array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       "  \n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       "  \n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       "  \n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       "  \n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]]]),\n",
       "  array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo numpy based auto differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Matmul:\n",
    "    def __init__(self):\n",
    "        self.mem = {}\n",
    "        \n",
    "    def forward(self, x, W):\n",
    "        h = np.matmul(x, W)\n",
    "        self.mem={'x': x, 'W':W}\n",
    "        return h\n",
    "    \n",
    "    def backward(self, grad_y):\n",
    "        '''\n",
    "        x: shape(N, d)\n",
    "        w: shape(d, d')\n",
    "        grad_y: shape(N, d')\n",
    "        '''\n",
    "        x = self.mem['x']\n",
    "        W = self.mem['W']\n",
    "        \n",
    "        ####################\n",
    "        '''计算矩阵乘法的对应的梯度'''\n",
    "        ####################\n",
    "        \n",
    "        grad_x = np.dot(grad_y, W.T)# shape(N, d'), shape(d', d)\n",
    "        \n",
    "        grad_W = np.dot(x.T, grad_y) # shape(d, N), shape(N, d')\n",
    "        \n",
    "        return grad_x, grad_W\n",
    "\n",
    "\n",
    "\n",
    "# x = max(0, x)\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mem = {}\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.mem['x']=x\n",
    "        return np.where(x > 0, x, np.zeros_like(x))\n",
    "    \n",
    "    def backward(self, grad_y):\n",
    "        '''\n",
    "        grad_y: same shape as x\n",
    "        '''\n",
    "        ####################\n",
    "        '''计算relu 激活函数对应的梯度'''\n",
    "        ####################\n",
    "        \n",
    "        tmp = self.mem['x'] > 0\n",
    "        \n",
    "        grad_x = tmp.astype(np.float32) * grad_y\n",
    "        \n",
    "        return grad_x\n",
    "\n",
    "class Softmax:\n",
    "    '''\n",
    "    softmax over last dimention\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.epsilon = 1e-12\n",
    "        self.mem = {}\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: shape(N, c)\n",
    "        '''\n",
    "        x_exp = np.exp(x)\n",
    "        partition = np.sum(x_exp, axis=1, keepdims=True) # shape(N, 1)\n",
    "        out = x_exp/(partition+self.epsilon) # shape(N, c)\n",
    "        \n",
    "        self.mem['out'] = out\n",
    "                \n",
    "        self.mem['x_exp'] = x_exp\n",
    "        return out\n",
    "    \n",
    "    def backward(self, grad_y):\n",
    "        '''\n",
    "        grad_y: same shape as x\n",
    "        '''\n",
    "        s = self.mem['out']\n",
    "                \n",
    "        sisj = np.matmul(np.expand_dims(s,axis=2), np.expand_dims(s, axis=1)) # shape(N, c, 1), shape(N, 1, c)\n",
    "        g_y_exp = np.expand_dims(grad_y, axis=1) # shape(N, 1, c)\n",
    "        \n",
    "        tmp = np.matmul(g_y_exp, sisj) # shape(N, 1, c)\n",
    "        tmp = np.squeeze(tmp, axis=1) # shape(N, c)\n",
    "        tmp = -tmp+grad_y*s \n",
    "        return tmp\n",
    "    \n",
    "class Log:\n",
    "    '''\n",
    "    softmax over last dimention\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.epsilon = 1e-12\n",
    "        self.mem = {}\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: shape(N, c)\n",
    "        '''\n",
    "        out = np.log(x+self.epsilon)\n",
    "        \n",
    "        self.mem['x'] = x\n",
    "        return out\n",
    "    \n",
    "    def backward(self, grad_y):\n",
    "        '''\n",
    "        grad_y: same shape as x\n",
    "        '''\n",
    "        x = self.mem['x']\n",
    "        \n",
    "        return 1./(x+1e-12) * grad_y\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.         2.40920946 0.         0.         0.         0.        ]\n",
      " [2.79585553 0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         1.09808535 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         1.06322662]\n",
      " [4.14786806 0.         0.         0.         0.         0.        ]], shape=(5, 6), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0.         2.40920946 0.         0.         0.         0.        ]\n",
      " [2.79585553 0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         1.09808535 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         1.06322662]\n",
      " [4.14786806 0.         0.         0.         0.         0.        ]], shape=(5, 6), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "# x = np.random.normal(size=[5, 6])\n",
    "# W = np.random.normal(size=[6, 4])\n",
    "# aa = Matmul()\n",
    "# out = aa.forward(x, W) # shape(5, 4)\n",
    "# grad = aa.backward(np.ones_like(out))\n",
    "# print (grad)\n",
    "\n",
    "# with tf.GradientTape() as tape:\n",
    "#     x, W = tf.constant(x), tf.constant(W)\n",
    "#     tape.watch(x)\n",
    "#     y = tf.matmul(x, W)\n",
    "#     loss = tf.reduce_sum(y)\n",
    "#     grads = tape.gradient(loss, x)\n",
    "#     print (grads)\n",
    "\n",
    "# x = np.random.normal(size=[5, 6])\n",
    "# aa = Relu()\n",
    "# out = aa.forward(x) # shape(5, 4)\n",
    "# grad = aa.backward(np.ones_like(out))\n",
    "# print (grad)\n",
    "\n",
    "# with tf.GradientTape() as tape:\n",
    "#     x= tf.constant(x)\n",
    "#     tape.watch(x)\n",
    "#     y = tf.nn.relu(x)\n",
    "#     loss = tf.reduce_sum(y)\n",
    "#     grads = tape.gradient(loss, x)\n",
    "#     print (grads)\n",
    "\n",
    "# x = np.random.normal(size=[5, 6], scale=5.0, loc=1)\n",
    "# label = np.zeros_like(x)\n",
    "# label[0, 1]=1.\n",
    "# label[1, 0]=1\n",
    "# label[1, 1]=1\n",
    "# label[2, 3]=1\n",
    "# label[3, 5]=1\n",
    "# label[4, 0]=1\n",
    "# print(label)\n",
    "# aa = Softmax()\n",
    "# out = aa.forward(x) # shape(5, 6)\n",
    "# grad = aa.backward(label)\n",
    "# print (grad)\n",
    "\n",
    "# with tf.GradientTape() as tape:\n",
    "#     x= tf.constant(x)\n",
    "#     tape.watch(x)\n",
    "#     y = tf.nn.softmax(x)\n",
    "#     loss = tf.reduce_sum(y*label)\n",
    "#     grads = tape.gradient(loss, x)\n",
    "#     print (grads)\n",
    "\n",
    "x = np.random.rand(5, 6)\n",
    "aa = Log()\n",
    "out = aa.forward(x) # shape(5, 4)\n",
    "grad = aa.backward(label)\n",
    "print (grad)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    x= tf.constant(x)\n",
    "    tape.watch(x)\n",
    "    y = tf.math.log(x)\n",
    "    loss = tf.reduce_sum(y*label)\n",
    "    grads = tape.gradient(loss, x)\n",
    "    print (grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Gradient Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00000000e+00 6.76312100e+03 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [3.04652832e+06 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 5.14210153e+02\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 1.22527188e+10]\n",
      " [1.00138727e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]]\n",
      "----------------------------------------\n",
      "[[0.00000000e+00 6.76312105e+03 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [3.04653760e+06 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 5.14210153e+02\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 1.24047102e+10]\n",
      " [1.00138727e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "label = np.zeros_like(x)\n",
    "label[0, 1]=1.\n",
    "label[1, 0]=1\n",
    "label[2, 3]=1\n",
    "label[3, 5]=1\n",
    "label[4, 0]=1\n",
    "\n",
    "x = np.random.normal(size=[5, 6])\n",
    "W1 = np.random.normal(size=[6, 5])\n",
    "W2 = np.random.normal(size=[5, 6])\n",
    "\n",
    "mul_h1 = Matmul()\n",
    "mul_h2 = Matmul()\n",
    "relu = Relu()\n",
    "softmax = Softmax()\n",
    "log = Log()\n",
    "\n",
    "h1 = mul_h1.forward(x, W1) # shape(5, 4)\n",
    "h1_relu = relu.forward(h1)\n",
    "h2 = mul_h2.forward(h1_relu, W2)\n",
    "h2_soft = softmax.forward(h2)\n",
    "h2_log = log.forward(h2_soft)\n",
    "\n",
    "\n",
    "h2_log_grad = log.backward(label)\n",
    "h2_soft_grad = softmax.backward(h2_log_grad)\n",
    "h2_grad, W2_grad = mul_h2.backward(h2_soft_grad)\n",
    "h1_relu_grad = relu.backward(h2_grad)\n",
    "h1_grad, W1_grad = mul_h1.backward(h1_relu_grad)\n",
    "\n",
    "print(h2_log_grad)\n",
    "print('--'*20)\n",
    "# print(W2_grad)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    x, W1, W2, label = tf.constant(x), tf.constant(W1), tf.constant(W2), tf.constant(label)\n",
    "    tape.watch(W1)\n",
    "    tape.watch(W2)\n",
    "    h1 = tf.matmul(x, W1)\n",
    "    h1_relu = tf.nn.relu(h1)\n",
    "    h2 = tf.matmul(h1_relu, W2)\n",
    "    prob = tf.nn.softmax(h2)\n",
    "    log_prob = tf.math.log(prob)\n",
    "    loss = tf.reduce_sum(label * log_prob)\n",
    "    grads = tape.gradient(loss, [prob])\n",
    "    print (grads[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myModel:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.W1 = np.random.normal(size=[28*28+1, 100])\n",
    "        self.W2 = np.random.normal(size=[100, 10])\n",
    "        \n",
    "        self.mul_h1 = Matmul()\n",
    "        self.mul_h2 = Matmul()\n",
    "        self.relu = Relu()\n",
    "        self.softmax = Softmax()\n",
    "        self.log = Log()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 28*28)\n",
    "        bias = np.ones(shape=[x.shape[0], 1]) # shape(N, 1)\n",
    "        x = np.concatenate([x, bias], axis=1) # shape(N, 28*28+1)\n",
    "        \n",
    "        self.h1 = self.mul_h1.forward(x, self.W1) # shape(5, 4)，神经网络的第二层（输入层为第一层）\n",
    "        self.h1_relu = self.relu.forward(self.h1) # 神经网络第一层的激活函数为relu函数\n",
    "        self.h2 = self.mul_h2.forward(self.h1_relu, self.W2) # 神经网络的第三层\n",
    "        self.h2_soft = self.softmax.forward(self.h2) # 神经网络的第三层的激活函数维softmax函数\n",
    "        self.h2_log = self.log.forward(self.h2_soft)\n",
    "            \n",
    "    def backward(self, label):\n",
    "        self.h2_log_grad = self.log.backward(-label)\n",
    "        self.h2_soft_grad = self.softmax.backward(self.h2_log_grad)\n",
    "        self.h2_grad, self.W2_grad = self.mul_h2.backward(self.h2_soft_grad)\n",
    "        self.h1_relu_grad = self.relu.backward(self.h2_grad)\n",
    "        self.h1_grad, self.W1_grad = self.mul_h1.backward(self.h1_relu_grad)\n",
    "        \n",
    "model = myModel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(log_prob, labels):\n",
    "     return np.mean(np.sum(-log_prob*labels, axis=1))\n",
    "    \n",
    "\n",
    "def compute_accuracy(log_prob, labels):\n",
    "    predictions = np.argmax(log_prob, axis=1)\n",
    "    truth = np.argmax(labels, axis=1)\n",
    "    return np.mean(predictions==truth)\n",
    "\n",
    "def train_one_step(model, x, y):\n",
    "    model.forward(x)\n",
    "    model.backward(y)\n",
    "    model.W1 -= 1e-5* model.W1_grad\n",
    "    model.W2 -= 1e-5* model.W2_grad\n",
    "    loss = compute_loss(model.h2_log, y)\n",
    "    accuracy = compute_accuracy(model.h2_log, y)\n",
    "    return loss, accuracy\n",
    "\n",
    "def test(model, x, y):\n",
    "    model.forward(x)\n",
    "    loss = compute_loss(model.h2_log, y)\n",
    "    accuracy = compute_accuracy(model.h2_log, y)\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "a, b = mnist_dataset()\n",
    "print(a[0].shape, b[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实际训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 : loss 23.904051174738896 ; accuracy 0.10295\n",
      "epoch 1 : loss 22.806400932570515 ; accuracy 0.14161666666666667\n",
      "epoch 2 : loss 22.100111542044562 ; accuracy 0.16695\n",
      "epoch 3 : loss 21.509961572197696 ; accuracy 0.18755\n",
      "epoch 4 : loss 20.979620446901052 ; accuracy 0.20748333333333333\n",
      "epoch 5 : loss 20.515692854913112 ; accuracy 0.22366666666666668\n",
      "epoch 6 : loss 20.075647479297572 ; accuracy 0.23848333333333332\n",
      "epoch 7 : loss 19.662329961052542 ; accuracy 0.2535\n",
      "epoch 8 : loss 19.267314336474815 ; accuracy 0.2683\n",
      "epoch 9 : loss 18.886144154375366 ; accuracy 0.27873333333333333\n",
      "epoch 10 : loss 18.452223774013234 ; accuracy 0.29481666666666667\n",
      "epoch 11 : loss 17.64581925201698 ; accuracy 0.31856666666666666\n",
      "epoch 12 : loss 17.320516294958807 ; accuracy 0.3388333333333333\n",
      "epoch 13 : loss 16.104623628794823 ; accuracy 0.37278333333333336\n",
      "epoch 14 : loss 15.258359882017869 ; accuracy 0.40675\n",
      "epoch 15 : loss 14.452309656944903 ; accuracy 0.4295833333333333\n",
      "epoch 16 : loss 13.99463250694171 ; accuracy 0.4578\n",
      "epoch 17 : loss 13.567895935915047 ; accuracy 0.4714\n",
      "epoch 18 : loss 13.307841710567766 ; accuracy 0.48395\n",
      "epoch 19 : loss 13.071296873592821 ; accuracy 0.49146666666666666\n",
      "epoch 20 : loss 12.851741300082546 ; accuracy 0.49955\n",
      "epoch 21 : loss 12.606288491233082 ; accuracy 0.50805\n",
      "epoch 22 : loss 12.341523861307843 ; accuracy 0.5142333333333333\n",
      "epoch 23 : loss 12.116838113918524 ; accuracy 0.52165\n",
      "epoch 24 : loss 11.880506514403354 ; accuracy 0.5275\n",
      "epoch 25 : loss 11.659626533217738 ; accuracy 0.5352666666666667\n",
      "epoch 26 : loss 11.413601681389665 ; accuracy 0.5437166666666666\n",
      "epoch 27 : loss 11.201190561573817 ; accuracy 0.55155\n",
      "epoch 28 : loss 10.938607964419228 ; accuracy 0.5611333333333334\n",
      "epoch 29 : loss 10.683433220194694 ; accuracy 0.5672333333333334\n",
      "epoch 30 : loss 10.299884275540439 ; accuracy 0.5766\n",
      "epoch 31 : loss 9.896850097834713 ; accuracy 0.5803666666666667\n",
      "epoch 32 : loss 9.600297182596014 ; accuracy 0.59355\n",
      "epoch 33 : loss 9.75564513835479 ; accuracy 0.5803166666666667\n",
      "epoch 34 : loss 9.961844572470344 ; accuracy 0.5916\n",
      "epoch 35 : loss 9.063504012235262 ; accuracy 0.6122666666666666\n",
      "epoch 36 : loss 8.153920567204297 ; accuracy 0.6426333333333333\n",
      "epoch 37 : loss 7.80988599614339 ; accuracy 0.6589\n",
      "epoch 38 : loss 7.729210014471861 ; accuracy 0.6603666666666667\n",
      "epoch 39 : loss 7.7817302160242265 ; accuracy 0.6705833333333333\n",
      "epoch 40 : loss 6.636770887688906 ; accuracy 0.7072333333333334\n",
      "epoch 41 : loss 6.455506984259472 ; accuracy 0.7115333333333334\n",
      "epoch 42 : loss 6.586317528106473 ; accuracy 0.7112\n",
      "epoch 43 : loss 6.542911879590402 ; accuracy 0.7093333333333334\n",
      "epoch 44 : loss 6.8021928801551494 ; accuracy 0.70685\n",
      "epoch 45 : loss 5.969293924124003 ; accuracy 0.7356833333333334\n",
      "epoch 46 : loss 5.909891053008453 ; accuracy 0.7378333333333333\n",
      "epoch 47 : loss 5.9534325339699645 ; accuracy 0.7369666666666667\n",
      "epoch 48 : loss 6.0652284077057566 ; accuracy 0.73325\n",
      "epoch 49 : loss 5.9151633376897035 ; accuracy 0.7385666666666667\n",
      "test loss 5.851797521698983 ; accuracy 0.7421\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = mnist_dataset()\n",
    "train_label = np.zeros(shape=[train_data[0].shape[0], 10])\n",
    "test_label = np.zeros(shape=[test_data[0].shape[0], 10])\n",
    "train_label[np.arange(train_data[0].shape[0]), np.array(train_data[1])] = 1.\n",
    "test_label[np.arange(test_data[0].shape[0]), np.array(test_data[1])] = 1.\n",
    "\n",
    "for epoch in range(50):\n",
    "    loss, accuracy = train_one_step(model, train_data[0], train_label)\n",
    "    print('epoch', epoch, ': loss', loss, '; accuracy', accuracy)\n",
    "loss, accuracy = test(model, test_data[0], test_label)\n",
    "\n",
    "print('test loss', loss, '; accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
