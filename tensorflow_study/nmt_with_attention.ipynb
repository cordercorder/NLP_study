{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_GPU:1', device_type='XLA_GPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_GPU:2', device_type='XLA_GPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_GPU:3', device_type='XLA_GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.experimental.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[2], \"GPU\")\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n",
    "        print(logical_gpus)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
      "2646016/2638744 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "path_to_zip = tf.keras.utils.get_file(\"/data/rrjin/corpus_data/spa-eng.zip\", origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\", extract=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/rrjin/corpus_data/spa-eng.zip'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = os.path.dirname(path_to_zip) + \"/spa-eng/spa.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/rrjin/corpus_data/spa-eng/spa.txt'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    return \"\".join(c for c in unicodedata.normalize(\"NFD\", s) if unicodedata.category(c) != \"Mn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "    # 在单词与跟在其后的标点符号之间插入一个空格\n",
    "    # 例如： \"he is a boy.\" => \"he is a boy .\"\n",
    "    # 参考：https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    \n",
    "    # 除了 (a-z, A-Z, \".\", \"?\", \"!\", \",\")，将所有字符替换为空格\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "    \n",
    "    w = w.rstrip().strip()\n",
    "\n",
    "    # 给句子加上开始和结束标记\n",
    "    # 以便模型知道何时开始和结束预测\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sentence = u\"May I borrow this book?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> may i borrow this book ? <end>\n"
     ]
    }
   ],
   "source": [
    "print(preprocess_sentence(en_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_sentence = u\"¿Puedo tomar prestado este libro?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> ¿ puedo tomar prestado este libro ? <end>\n"
     ]
    }
   ],
   "source": [
    "print(preprocess_sentence(sp_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<start> \\xc2\\xbf puedo tomar prestado este libro ? <end>'\n"
     ]
    }
   ],
   "source": [
    "print(preprocess_sentence(sp_sentence).encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(path, num_examples):\n",
    "    lines = io.open(path, encoding=\"utf-8\").read().strip().split(\"\\n\")\n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split(\"\\t\")] for l in lines[:num_examples]]\n",
    "    return zip(*word_pairs) # src, tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<zip at 0x7f8edc7caeb0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_dataset(path_to_file, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "en, sp = create_dataset(path_to_file, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> vete . <end>'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=\"\")\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    \n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding=\"post\")\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, num_examples=None):\n",
    "    tgt_lang, src_lang = create_dataset(path, num_examples)\n",
    "    \n",
    "    src_tensor, src_lang_tokenizer = tokenize(src_lang)\n",
    "    tgt_tensor, tgt_lang_tokenizer = tokenize(tgt_lang)\n",
    "    \n",
    "    return src_tensor, tgt_tensor, src_lang_tokenizer, tgt_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = 30000\n",
    "src_tensor, tgt_tensor, src_lang_tokenizer, tgt_lang_tokenizer = load_dataset(path_to_file, num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_src, max_length_tgt = max_length(src_tensor), max_length(tgt_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tensor_train, src_tensor_val, tgt_tensor_train, tgt_tensor_val = train_test_split(src_tensor, tgt_tensor, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000 6000 24000 6000\n"
     ]
    }
   ],
   "source": [
    "print(len(src_tensor_train), len(src_tensor_val), len(tgt_tensor_train), len(tgt_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = len(src_tensor_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = len(src_tensor_train) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = len(src_lang_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_vocab_size = len(tgt_lang_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 16)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((src_tensor_train, tgt_tensor_train)).shuffle(buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_size):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # input is :(batch_size, timesteps, input_dim)\n",
    "        # if return_sequence is true: output is: (batch_size, timesteps, units)\n",
    "        # else output is: (batch_size, units)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units, return_sequences=True, return_state=True, recurrent_initializer=\"glorot_uniform\")\n",
    "    \n",
    "    def call(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: 2D tensor with shape: (batch_size, input_length)\n",
    "            output_shape of x: 3D tensor with shape: (batch_size, input_length, embedding_dim)\n",
    "        \"\"\"\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        return output, state\n",
    "        \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_src_batch, example_tgt_batch = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 16]), TensorShape([64, 11]))"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_src_batch.shape, example_tgt_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape :(batch_size, input_length, units) (64, 16, 1024)\n",
      "Encoder hidden shape :(batch_size units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(src_vocab_size, embedding_dim, units, batch_size)\n",
    "\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_src_batch, sample_hidden)\n",
    "print(\"Encoder output shape :(batch_size, input_length, units) {}\".format(sample_output.shape))\n",
    "print(\"Encoder hidden shape :(batch_size units) {}\".format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 16), (64, 11)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units):\n",
    "        \n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    \n",
    "    def call(self, hs, ht):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            ht: (batch, input_length, units_of_encoder) output sequence\n",
    "            hs: (batch, units_of_encoder) last hidden states\n",
    "        \"\"\"\n",
    "        hs = tf.expand_dims(hs, axis=1)  # (batch, 1, units_of_encoder)\n",
    "        \n",
    "        # (batch, input_length, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(ht) + self.W2(hs)))  # broadcasting will hapen\n",
    "        \n",
    "        # (batch, input_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        # context_vector (batch, input_length, units_of_encoder)\n",
    "        context_vector = attention_weights * ht # broadcasting will hapen\n",
    "        \n",
    "        # context_vecor (batch, units_of_encoder)\n",
    "        context_vector = tf.math.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape:(batch, units_of_encoder) (64, 1024)\n",
      "Attention weight shape:(batch, input_length, 1) (64, 16, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "print(\"Attention result shape:(batch, units_of_encoder) {}\".format(attention_result.shape))\n",
    "print(\"Attention weight shape:(batch, input_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_size):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units, return_sequences=True, return_state=True, recurrent_initializer=\"glorot_uniform\")\n",
    "        \n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "        \n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, 1)\n",
    "        \"\"\"\n",
    "        \n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        \n",
    "        # (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # (batch_size, 1, units_of_encoder + embedding_dim)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, axis=1), x], axis=-1)\n",
    "        \n",
    "        # output: (batch_size, 1, dec_units)\n",
    "        # state: (batch_size, dec_units)\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        output = tf.reshape(output, [-1, output.shape[2]])\n",
    "        \n",
    "        # (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape (bathch_size, vocab) (64, 4935)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(tgt_vocab_size, embedding_dim, units, batch_size)\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((64, 1)), sample_hidden, sample_output)\n",
    "print(\"Decoder output shape (bathch_size, vocab) {}\".format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    \n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"./training_check_points\"\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer = optimizer, encoder = encoder, decoder = decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(src, tgt, enc_hidden):\n",
    "    loss = 0\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        enc_output, enc_hidden = encoder(src, enc_hidden)\n",
    "        \n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([tgt_lang_tokenizer.word_index[\"<start>\"]] * batch_size, axis=1)\n",
    "        \n",
    "        for t in range(1, tgt.shape[1]):\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            \n",
    "            loss += loss_function(tgt[:, t], predictions)\n",
    "            \n",
    "            # teacher forcing\n",
    "            dec_input = tf.expand_dims(tgt[:, t], axis=1)\n",
    "    \n",
    "    batch_loss = (loss / tgt.shape[1])\n",
    "    \n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (64, 16) (64, 11)\n",
      "1 (64, 16) (64, 11)\n",
      "2 (64, 16) (64, 11)\n",
      "3 (64, 16) (64, 11)\n",
      "4 (64, 16) (64, 11)\n",
      "5 (64, 16) (64, 11)\n",
      "6 (64, 16) (64, 11)\n",
      "7 (64, 16) (64, 11)\n",
      "8 (64, 16) (64, 11)\n",
      "9 (64, 16) (64, 11)\n",
      "10 (64, 16) (64, 11)\n",
      "11 (64, 16) (64, 11)\n",
      "12 (64, 16) (64, 11)\n",
      "13 (64, 16) (64, 11)\n",
      "14 (64, 16) (64, 11)\n",
      "15 (64, 16) (64, 11)\n",
      "16 (64, 16) (64, 11)\n",
      "17 (64, 16) (64, 11)\n",
      "18 (64, 16) (64, 11)\n",
      "19 (64, 16) (64, 11)\n",
      "20 (64, 16) (64, 11)\n",
      "21 (64, 16) (64, 11)\n",
      "22 (64, 16) (64, 11)\n",
      "23 (64, 16) (64, 11)\n",
      "24 (64, 16) (64, 11)\n",
      "25 (64, 16) (64, 11)\n",
      "26 (64, 16) (64, 11)\n",
      "27 (64, 16) (64, 11)\n",
      "28 (64, 16) (64, 11)\n",
      "29 (64, 16) (64, 11)\n",
      "30 (64, 16) (64, 11)\n",
      "31 (64, 16) (64, 11)\n",
      "32 (64, 16) (64, 11)\n",
      "33 (64, 16) (64, 11)\n",
      "34 (64, 16) (64, 11)\n",
      "35 (64, 16) (64, 11)\n",
      "36 (64, 16) (64, 11)\n",
      "37 (64, 16) (64, 11)\n",
      "38 (64, 16) (64, 11)\n",
      "39 (64, 16) (64, 11)\n",
      "40 (64, 16) (64, 11)\n",
      "41 (64, 16) (64, 11)\n",
      "42 (64, 16) (64, 11)\n",
      "43 (64, 16) (64, 11)\n",
      "44 (64, 16) (64, 11)\n",
      "45 (64, 16) (64, 11)\n",
      "46 (64, 16) (64, 11)\n",
      "47 (64, 16) (64, 11)\n",
      "48 (64, 16) (64, 11)\n",
      "49 (64, 16) (64, 11)\n",
      "50 (64, 16) (64, 11)\n"
     ]
    }
   ],
   "source": [
    "for i, (x, y) in enumerate(dataset.as_numpy_iterator()):\n",
    "    print(i, x.shape, y.shape)\n",
    "    if i == 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch 0 Loss 4.6630\n",
      "Epoch 0 Batch 100 Loss 2.1399\n",
      "Epoch 0 Batch 200 Loss 1.8757\n",
      "Epoch 0 Batch 300 Loss 1.7250\n",
      "Time taken for 1 epoch 40.050865650177 sec \n",
      "\n",
      "Epoch 1 Batch 0 Loss 1.5574\n",
      "Epoch 1 Batch 100 Loss 1.3636\n",
      "Epoch 1 Batch 200 Loss 1.4104\n",
      "Epoch 1 Batch 300 Loss 1.2972\n",
      "Time taken for 1 epoch 31.20362663269043 sec \n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.0315\n",
      "Epoch 2 Batch 100 Loss 1.0685\n",
      "Epoch 2 Batch 200 Loss 1.0386\n",
      "Epoch 2 Batch 300 Loss 0.9027\n",
      "Time taken for 1 epoch 30.90855097770691 sec \n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.7472\n",
      "Epoch 3 Batch 100 Loss 0.6549\n",
      "Epoch 3 Batch 200 Loss 0.7085\n",
      "Epoch 3 Batch 300 Loss 0.6825\n",
      "Time taken for 1 epoch 31.21520733833313 sec \n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.4659\n",
      "Epoch 4 Batch 100 Loss 0.3772\n",
      "Epoch 4 Batch 200 Loss 0.4765\n",
      "Epoch 4 Batch 300 Loss 0.4158\n",
      "Time taken for 1 epoch 30.909621477127075 sec \n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.3094\n",
      "Epoch 5 Batch 100 Loss 0.3266\n",
      "Epoch 5 Batch 200 Loss 0.3426\n",
      "Epoch 5 Batch 300 Loss 0.3026\n",
      "Time taken for 1 epoch 31.094359636306763 sec \n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.2309\n",
      "Epoch 6 Batch 100 Loss 0.2367\n",
      "Epoch 6 Batch 200 Loss 0.1818\n",
      "Epoch 6 Batch 300 Loss 0.1881\n",
      "Time taken for 1 epoch 30.923243522644043 sec \n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.1522\n",
      "Epoch 7 Batch 100 Loss 0.2083\n",
      "Epoch 7 Batch 200 Loss 0.0951\n",
      "Epoch 7 Batch 300 Loss 0.1747\n",
      "Time taken for 1 epoch 31.081210374832153 sec \n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.0850\n",
      "Epoch 8 Batch 100 Loss 0.0758\n",
      "Epoch 8 Batch 200 Loss 0.1269\n",
      "Epoch 8 Batch 300 Loss 0.0909\n",
      "Time taken for 1 epoch 30.868971586227417 sec \n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.0793\n",
      "Epoch 9 Batch 100 Loss 0.0827\n",
      "Epoch 9 Batch 200 Loss 0.0982\n",
      "Epoch 9 Batch 300 Loss 0.1354\n",
      "Time taken for 1 epoch 31.18552803993225 sec \n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 10\n",
    "    \n",
    "    for batch, (src, tgt) in enumerate(dataset.as_numpy_iterator()):\n",
    "        batch_loss = train_step(src, tgt, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print(\"Epoch {} Batch {} Loss {:.4f}\".format(epoch, batch, batch_loss.numpy()))\n",
    "    \n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    \n",
    "    print(\"Time taken for 1 epoch {} sec \\n\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(src_lang_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_tgt, max_length_src))\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    \n",
    "    input_tensor = [src_lang_tokenizer.word_index.get(word, 0) for word in sentence.split()]\n",
    "    input_tensor = tf.keras.preprocessing.sequence.pad_sequences([input_tensor], maxlen=max_length_src, padding=\"post\")\n",
    "    \n",
    "    input_tensor = tf.convert_to_tensor(input_tensor)\n",
    "    \n",
    "    result = \"\"\n",
    "    \n",
    "    hidden = tf.zeros((1, units))\n",
    "    enc_out, enc_hidden = encoder(input_tensor, hidden)\n",
    "    \n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([tgt_lang_tokenizer.word_index[\"<start>\"]], 0)\n",
    "    \n",
    "    for t in range(max_length_tgt):\n",
    "        \n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        \n",
    "        attention_weights = tf.reshape(attention_weights, (-1,))\n",
    "        attention_plot[t] = attention_weights\n",
    "        \n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        \n",
    "#         print(predicted_id)\n",
    "        \n",
    "        result += tgt_lang_tokenizer.index_word[predicted_id] + \" \"\n",
    "#         print(result)\n",
    "        \n",
    "        if tgt_lang_tokenizer.index_word[predicted_id] == \"<end>\":\n",
    "            return result, sentence, attention_plot\n",
    "        \n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "#         print(dec_input.shape)\n",
    "        \n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap=\"viridis\")\n",
    "    \n",
    "    fontdict = {\"fontsize\":14}\n",
    "    \n",
    "    ax.set_xticklabels([\"\"]+sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([\"\"]+predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    \n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "    \n",
    "    print(\"Input: {}\".format(sentence))\n",
    "    print(\"Predicted translation: {}\".format(result))\n",
    "    \n",
    "    attention_plot = attention_plot[:len(result.split()), :len(sentence.split())]\n",
    "    plot_attention(attention_plot, sentence.split(), result.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> hace mucho frio aqui . <end>\n",
      "Predicted translation: it s very cold here . <end> \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(u'hace mucho frio aqui.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
